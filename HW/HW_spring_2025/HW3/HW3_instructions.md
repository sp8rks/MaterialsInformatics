# HW3: Deep Learning - VAE & CrabNet (Spring 2026)

## Overview
**Jupyter Notebook Provided:** A starter Jupyter Notebook, `HW3_spring2026_final.ipynb`, is provided in the assignment folder. You are encouraged to use it to develop and test your code for each task.

The goal of this assignment is to implement and train deep learning models for materials science applications, including generative models (VAE) and transformer-based architectures (CrabNet).

**Assignment Structure:**
- **Part 1:** Variational Autoencoder (VAE) for microstructure images (~4-5 hours)
- **Part 2:** CrabNet transformer model for materials property prediction (~3-4 hours)

**Total Estimated Time:** 7-9 hours (Note: Training times vary based on CPU vs GPU)

**Submission:** Submit your Python code file(s) that complete all tasks below.

**Data Requirements:**
- **Datasets:** All data for this assignment is located in the `HW3/data/` directory.
- The microstructure images for Part 1 are in the `HW3/data/micrographs/` sub-folder.
- The shear modulus dataset for Part 2 is the file `HW3/data/cp_data_cleaned.csv`.

---

## Prerequisites: PyTorch Setup and Troubleshooting

This guide provides step-by-step instructions and a Frequently Asked Questions (FAQ) section to help you install PyTorch.

#### **A. Step-by-Step Setup Guide**

**Step 1: Go to the Official PyTorch Website**
The correct installation command depends on your specific system (OS, GPU). The official website generates the exact command you need.
*   **Action:** Open your browser and go to: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)

**Step 2: Select Your Configuration**
On the website, you will see a configuration matrix.
*   **PyTorch Build:** Choose **Stable**.
*   **Your OS:** Choose Mac, Linux, or Windows.
*   **Package:** Choose **Pip** (since we are using `uv`, which works with Pip packages).
*   **Language:** Choose **Python**.
*   **Compute Platform:** This is the most important choice.
    *   **For CPU (Recommended if unsure):** If you don't have a powerful NVIDIA graphics card, or don't want to deal with driver installations, choose **CPU**. This will work on any computer.
    *   **For GPU (Advanced):** If you have an NVIDIA GPU, you can choose a CUDA version. This will make training much faster but requires correct driver setup. You can check your CUDA version by running `nvidia-smi` in a terminal. If that command fails, you should select CPU.

**Step 3: Install with UV**
*   **Action:**
    1.  Activate your virtual environment for HW3 (`source .venv/bin/activate`).
    2.  Copy the command generated by the website. It will look something like `pip3 install torch torchvision torchaudio`.
    3.  Replace `pip3` with `uv pip`. For example:
        ```bash
        # Example command for CPU-only on Mac/Windows
        uv pip install torch torchvision torchaudio
        ```
*   **Verification:** Create and run a small Python script:
    ```python
    import torch
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    ```

#### **B. Troubleshooting FAQ (HW3)**

*   **Q: I get `ModuleNotFoundError: No module named 'torch'`.**
    *   **A:** Just like in HW1, this means your virtual environment is likely not active. Make sure your terminal prompt starts with `(.venv)`. If not, activate it.

*   **Q: My script says `torch.cuda.is_available()` is `False`.**
    *   **A:** This means PyTorch cannot detect a compatible GPU.
        1.  **Do you have an NVIDIA GPU?** PyTorch GPU support is primarily for NVIDIA (via CUDA). AMD or Intel GPUs will not work for this.
        2.  **Did you install the CPU version?** If you selected "CPU" on the website, this is the expected output. Your code will still run, just on the CPU.
        3.  **Are your NVIDIA drivers up to date?** If you do have an NVIDIA GPU, make sure you have the latest drivers installed.
        4.  **Did you install the correct PyTorch build?** You may have accidentally installed the CPU-only version. Rerun the installation command from the PyTorch website, making sure to select a CUDA version.

*   **Q: I get a `CUDA out of memory` error when training.**
    *   **A:** This is a very common error. It means the model and the batch of data are too large to fit in your GPU's memory (VRAM).
    *   **The Fix:** The easiest solution is to **reduce the `batch_size`** in your `DataLoader`. Try cutting it in half (e.g., from 64 to 32, or 32 to 16) and run the training again.

*   **Q: Training is extremely slow!**
    *   **A:** If you are running on a CPU, this is expected. Deep learning is very computationally intensive. For debugging your code, you can reduce the `num_epochs` to a small number (like 2-3) so it finishes quickly. For the final submission, let it run for the required number of epochs, even if it takes a while.

---

## Part 1: Variational Autoencoder (VAE)

### Background
We are going to build a simple VAE model to work with microstructure images. The images are in a folder called `micrographs`. Make sure you downloaded this and use it as the training data.

---

### Task 1.1: Import Libraries
Import the relevant packages needed for this assignment.

**Required packages:**
```python
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import os
from PIL import Image
import glob
```

**Deliverables:**
- Code to import all necessary libraries
- Verify PyTorch is working correctly by printing torch.__version__

---

### Task 1.2: Load Images Function
Create a function that loads all your images and returns a list of paths to the images for each image.

**Requirements:**
- The final list will contain a path to each image in the folder
- The images in the dataset contain .png, .tif, and .tiff files
- Make sure to specify these extensions within the function

**Function signature:** `load_images(data_dir) -> [list of paths to the images]`

**Deliverables:**
- Define the `load_images(data_dir)` function
- Test it on the micrographs folder
- Print the number of images found

---

### Task 1.3: Create MicrographDataset Class
Create a class called `MicrographDataset` that inherits from PyTorch's Dataset class. This class is going to take in your image_paths (the output of the load_images function) and apply transformations to the images.

**Required transformations:**
- Resize to 32×32 pixels
- Normalization of the images to [0,1]
- Conversion to grayscale

**Hint:** Your class should have the following methods:
- `__init__(self, image_paths, transform)`
- `__len__(self)`
- `__getitem__(self, idx)`

**Deliverables:**
- Define the `MicrographDataset` class
- Create a dataset instance with appropriate transformations
- Print the length of the dataset
- Visualize a sample image to verify transformations

---

### Task 1.4: Implement Convolutional VAE
Create a simple VAE with latent dimension of 8. In this class, you should have an encoder and a decoder.

Implement the `ConvolutionalVAE` class, inheriting from nn.Module.

**Encoder Architecture:**
- Input: grayscale images of size 32×32
- First convolutional layer: 16 filters, kernel size 3, stride 2, padding 1
- ReLU activation
- Second convolutional layer: 32 filters, kernel size 3, stride 2, padding 1
- ReLU activation
- Flatten the output of the second convolutional layer
- Pass through two fully connected layers to get the mean (mu) and log variance (logvar)
- The size after flattening should be 32 × 8 × 8 = 2048
- Use linear layers to get mu and logvar (both should be of size latent_dim=8)

**Decoder Architecture:**
- Fully connected layer to reshape from latent space (8) to feature volume (32 × 8 × 8 = 2048)
- Reshape to (32, 8, 8)
- First transposed convolution: 16 filters, kernel size 3, stride 2, padding 1, output_padding=1
- ReLU activation
- Second transposed convolution: 1 filter, kernel size 3, stride 2, padding 1, output_padding=1
- Final activation: Sigmoid

**Required methods:**
- `encode(x)`: Processes input through the encoder and returns mu and logvar
- `reparameterize(mu, logvar)`: Applies the reparameterization trick for sampling
- `decode(z)`: Reconstructs images from latent vectors
- `forward(x)`: Combines the encode and decode methods

**Starter Code Template:**
```python
class ConvolutionalVAE(nn.Module):
    def __init__(self, latent_dim=8):
        super(ConvolutionalVAE, self).__init__()
        self.latent_dim = latent_dim

        # Encoder layers
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)
        self.fc_mu = nn.Linear(32 * 8 * 8, latent_dim)
        self.fc_logvar = nn.Linear(32 * 8 * 8, latent_dim)

        # Decoder layers
        self.fc_decode = nn.Linear(latent_dim, 32 * 8 * 8)
        self.deconv1 = nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1)
        self.deconv2 = nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1)

    def encode(self, x):
        # TODO: Implement encoder
        # Hint: Apply conv1, ReLU, conv2, ReLU, flatten, then compute mu and logvar
        pass

    def reparameterize(self, mu, logvar):
        # TODO: Implement reparameterization trick
        # Hint: std = exp(0.5 * logvar), then sample epsilon ~ N(0,1), return mu + std * epsilon
        pass

    def decode(self, z):
        # TODO: Implement decoder
        # Hint: fc_decode, reshape to (32, 8, 8), deconv1, ReLU, deconv2, sigmoid
        pass

    def forward(self, x):
        # TODO: Combine encode, reparameterize, and decode
        # Return reconstructed image, mu, and logvar
        pass
```

**Deliverables:**
- Complete the `ConvolutionalVAE` class by filling in the TODO sections
- Instantiate the model
- Print the model architecture to verify

---

### Task 1.5: Define VAE Loss Function
Define a VAE loss function called `vae_loss`.

**Requirements:**
- Calculate the binary cross-entropy (BCE) between the reconstructed and original images
- Calculate the KL divergence between the latent distribution and a standard normal distribution
- Combine these with a beta parameter = 0.1

**Formula:**
```
total_loss = BCE_loss + beta * KL_divergence
```

Where:
- BCE_loss = binary cross entropy between reconstruction and input
- KL_divergence = -0.5 * sum(1 + logvar - mu^2 - exp(logvar))

**Deliverables:**
- Define the `vae_loss(recon_x, x, mu, logvar, beta=0.1)` function
- Test it on dummy data to verify it works

---

### Task 1.6: Create Training Function
Create a train function called `train_vae` that trains the VAE model.

**Function parameters:**
- `model`: the VAE model
- `data_loader`: DataLoader with training data
- `num_epochs`: number of training epochs

**Requirements:**
- Use the Adam optimizer with learning rate = 1e-3
- Track the loss for each epoch
- Try different numbers of epochs and learning rates to see how the model performs

**Deliverables:**
- Define the `train_vae(model, data_loader, num_epochs)` function
- Include code to track and store losses
- Return the list of losses

---

### Task 1.7: Train the VAE
Run the training and plot the loss curve with respect to epochs.

**Requirements:**
- Create a DataLoader from your MicrographDataset
- Set an appropriate batch size (e.g., 32 or 64)
- Train for at least 20-50 epochs
- Plot the loss curve (epochs vs. loss)
- Save the trained model weights

**Deliverables:**
- Code to create DataLoader
- Code to train the model
- Plot the training loss curve and save as `vae_training_loss.png`
- Save model weights as `vae_model.pth`

---

### Task 1.8: Generate Similar Images
Create a function called `generate_similar_images()` that takes a trained VAE model and an original micrograph image, then generates multiple similar but non-identical variations of the original image by adding noise in the latent space.

**Requirements:**
- Encode the original image to get the mean and log variance
- Sample from the distribution to get a latent vector
- Add small amounts of noise to the latent vector
- Decode the noisy latent vectors to get new images
- Generate 5 similar images

**Function signature:** `generate_similar_images(model, original_image, num_images=5, noise_scale=0.1)`

**Deliverables:**
- Define the `generate_similar_images()` function
- Generate 5 similar images for a random image in the dataset
- Display the original and 5 generated images side by side
- Save the visualization as `generated_similar_images.png`

---

## Part 2: CrabNet Transformer Model

### Background
Now we are going to train the CrabNet model on a shear modulus dataset. CrabNet is a deep learning architecture specifically designed for materials property prediction using only the chemical formula as input.

**Resources:**
- **CrabNet Repository:** https://github.com/anthony-wang/CrabNet
- **CrabNet Documentation:** https://crabnet.readthedocs.io/en/latest/
- **Installation Guide:** https://crabnet.readthedocs.io/en/latest/installation.html
- **Quick Start Tutorial:** https://crabnet.readthedocs.io/en/latest/quickstart.html

**Installation:**
```bash
uv pip install crabnet
```

**Important Note:** CrabNet expects data in a specific format with columns named `'formula'` and `'target'`. Review the Quick Start Tutorial to understand the expected data structure.

---

### Task 2.1: Setup CrabNet
Load the shear modulus dataset from `data/cp_data_cleaned.csv` into a pandas DataFrame.

**Deliverables:**
- Code to import CrabNet
- Code to load the `cp_data_cleaned.csv` dataset
- Print the first few rows and dataset shape
- Verify the dataset has the required columns for CrabNet

---

### Task 2.2: Prepare Data
Prepare the dataset for training with CrabNet.

**Requirements:**
- Ensure your DataFrame has columns named `'formula'` and `'target'` (rename if necessary)
- Split the data into training (90%) and testing (10%) sets using a random split
- CrabNet handles featurization internally from chemical formulas - you do NOT need to use CBFV for this part

**Deliverables:**
- Code to prepare the data in the correct format
- Code to split the dataset
- Print the shapes of train and test sets
- Verify column names match CrabNet's requirements

---

### Task 2.3: Train CrabNet Model
Train the CrabNet model on the dataset and plot the loss curve with respect to epochs.

**Requirements:**
- Set up the CrabNet model with appropriate hyperparameters
- Train the model on the training set
- Track the training loss for each epoch
- Plot the loss curve

**Deliverables:**
- Code to initialize and train the CrabNet model
- Plot the training loss curve and save as `crabnet_training_loss.png`
- Save the trained model

---

### Task 2.4: Evaluate on Test Set
Test the model on the 10% test set and calculate the mean squared error (MSE).

**Deliverables:**
- Code to make predictions on the test set
- Calculate and print the MSE value
- Create a parity plot (predicted vs. actual)
- Save the parity plot as `crabnet_parity_plot.png`

---

### Task 2.5: Predict on New Materials
Use the trained model to predict the shear modulus on the following material compositions:

```python
new_materials = [
    "Fe2O3",
    "TiO2",
    "Al2O3",
    "SiO2",
    "ZnO",
    "CaTiO3",
    "Li4Ti5O12",
    "BaTiO3",
    "LiFePO4",
    "MgAl2O4"
]
```

**Deliverables:**
- Code to make predictions on the new materials
- Put the predicted values in a dictionary with the material name as the key and the predicted value as the value
- Print the dictionary in a nicely formatted way

**Expected output format:**
```
Shear Modulus Predictions:
Fe2O3: XXX.XX GPa
TiO2: XXX.XX GPa
...
```

---

## Common Mistakes and Tips

### Part 1: VAE
- **Mistake:** Forgetting to flatten the tensor before passing to fully connected layers
  - **Fix:** Use `x = x.view(x.size(0), -1)` or `x = torch.flatten(x, start_dim=1)`
- **Mistake:** Not matching tensor dimensions in the decoder
  - **Fix:** After the fully connected layer, reshape to `(batch_size, 32, 8, 8)` using `x.view(-1, 32, 8, 8)`
- **Mistake:** Using the wrong activation function in the decoder output
  - **Fix:** Final layer must use `torch.sigmoid()` to ensure output is in [0,1] range
- **Mistake:** Forgetting to put the model in `.train()` or `.eval()` mode
  - **Fix:** Use `model.train()` during training and `model.eval()` during evaluation/generation
- **Mistake:** Not detaching tensors when moving to numpy for visualization
  - **Fix:** Use `image.detach().cpu().numpy()` when converting PyTorch tensors to numpy

### PyTorch Specific
- **Mistake:** CUDA out of memory errors
  - **Fix:** Reduce `batch_size` in your DataLoader (try 16 or 32 instead of 64)
- **Mistake:** Training on CPU is extremely slow
  - **Reality:** This is expected. Reduce `num_epochs` to 5-10 for testing, then run full training overnight
- **Mistake:** Not zeroing gradients before backward pass
  - **Fix:** Always call `optimizer.zero_grad()` before `loss.backward()`
- **Mistake:** Trying to backward through a computation graph twice
  - **Fix:** Call `.detach()` on tensors you don't want to track gradients for

### Part 2: CrabNet
- **Mistake:** Not renaming columns to 'formula' and 'target'
  - **Fix:** CrabNet is very specific about column names - rename before training
- **Mistake:** Including chemical formulas with spaces or special characters
  - **Fix:** Clean formulas to standard format (e.g., "Fe2O3" not "Fe 2 O 3")
- **Mistake:** Not checking CrabNet documentation for API changes
  - **Fix:** The CrabNet API may differ from tutorial examples - always check the official docs
- **Mistake:** Expecting instant training
  - **Reality:** CrabNet training can take 10-30 minutes even on small datasets

### General Deep Learning
- **Mistake:** Not saving model checkpoints during training
  - **Fix:** Save your model after training completes using `torch.save(model.state_dict(), 'model.pth')`
- **Mistake:** Not monitoring loss curves during training
  - **Fix:** Print and plot loss every few epochs to catch training problems early
- **Mistake:** Using test data for hyperparameter tuning
  - **Fix:** Test set should ONLY be used for final evaluation

---

## Grading Rubric

**Part 1: VAE (50 points)**
- Tasks 1.1-1.3 (Setup and data loading): 10 points
- Task 1.4 (VAE architecture): 15 points
- Tasks 1.5-1.6 (Loss and training functions): 10 points
- Tasks 1.7-1.8 (Training and generation): 15 points

**Part 2: CrabNet (50 points)**
- Tasks 2.1-2.2 (Setup and data preparation): 10 points
- Task 2.3 (Training): 15 points
- Task 2.4 (Evaluation): 10 points
- Task 2.5 (Predictions on new materials): 15 points

**Total: 100 points**

---

## Submission Guidelines

**Important Note on File Naming:** Throughout this assignment, you are required to save data and plots to specific filenames. It is critical that you use these exact filenames, as your submission may be graded by scripts that look for these files. Failure to use the correct filenames may result in a loss of points.

1. **Code:** Submit Python file(s) named `hw3_yourname.py` (or `hw3_part1.py`, `hw3_part2.py`)
2. **Model Weights:** Include saved model files (`vae_model.pth`, CrabNet model files)
3. **Figures:** Include all generated plots (.png files)
4. **Output:** Include print statements showing your results
5. **Comments:** Add clear comments explaining your approach

**Required files:**
- `hw3_yourname.py` (or separate files for each part)
- `vae_model.pth` - Saved VAE model weights
- `vae_training_loss.png` - VAE training curve
- `generated_similar_images.png` - Generated microstructure variations
- `crabnet_training_loss.png` - CrabNet training curve
- `crabnet_parity_plot.png` - CrabNet test set parity plot
- CrabNet model files (if applicable)

**Important Notes:**
- Training deep learning models can take time. Start early!
- You may need GPU access for faster training (though CPU is fine for this assignment)
- Experiment with hyperparameters (learning rate, epochs, batch size) for better results
- Document any challenges you faced and how you solved them

**Deadline:** March 5, 2026 at 23:59