{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8178ad1",
   "metadata": {},
   "source": [
    "# Bayesian Optimization with Ax Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use Bayesian Optimization with the Ax library to optimize machine learning model parameters and other experiments efficiently. Ax is a powerful tool developed by Facebook for automating experimentation and optimization.\n",
    "\n",
    "## What is Bayesian Optimization?\n",
    "\n",
    "Bayesian Optimization is an efficient strategy for optimizing black-box functions that are expensive to evaluate. It's particularly well-suited for high-dimensional spaces and situations where sampling data is costly.\n",
    "\n",
    "## Introduction to Ax\n",
    "\n",
    "Ax is an accessible, modular, and efficient library that supports both gradient-based optimization and Bayesian Optimization. It is designed to automate the process of optimizing complex experiments, like tuning hyperparameters for machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d257042e",
   "metadata": {},
   "source": [
    "# Basic Ax Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407d5b7",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "After installing Ax, we need to import the necessary libraries to set up our optimization problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "749ea947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Ax libraries\n",
    "from ax.service.ax_client import AxClient\n",
    "from ax.utils.notebook.plotting import render, init_notebook_plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc5ac43",
   "metadata": {},
   "source": [
    "## Defining the Optimization Problem\n",
    "\n",
    "For this tutorial, we'll optimize a simple synthetic function as our objective. Our goal is to find the minimum value of this function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3520065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective_function(parameters):\n",
    "    x = parameters.get(\"x\")\n",
    "    y = parameters.get(\"y\")\n",
    "    objective_value = (x - 0.5)**2 + (y - 0.5)**2  # Simple convex function\n",
    "    return {\"objective_value\": (objective_value, 0.0)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7088a4bd",
   "metadata": {},
   "source": [
    "## Configuring Bayesian Optimization in Ax\n",
    "\n",
    "We will now configure the Bayesian Optimization process in Ax by setting up an experiment. The evaluation function calculates an objective value based on the parameters x and y. We are using a simple quadratic function for this problem. This function matches our originally defined objective function. The optimize function takes a few inputs. The first is a list of parameters to be optimized. Each is given a name, type, and optimization bounds. Next, you have to define an evaluation function for the model to use to evaluate the generated parameters. Next is the objective to be optimized. This is the name of the value. This needs to match the name defined in the objective function. Next, tell the model if it's trying to minimize or maximize. Lastly you can specify the number of trials it needs to run. The higher this number is the more accurate it will be. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d87a90ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 09-25 19:52:16] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 09-25 19:52:16] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter y. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 09-25 19:52:16] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='x', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='y', parameter_type=FLOAT, range=[0.0, 1.0])], parameter_constraints=[]).\n",
      "[INFO 09-25 19:52:17] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.\n",
      "[INFO 09-25 19:52:17] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=2 num_trials=None use_batch_trials=False\n",
      "[INFO 09-25 19:52:17] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=5\n",
      "[INFO 09-25 19:52:17] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=5\n",
      "[INFO 09-25 19:52:17] ax.modelbridge.dispatch_utils: `verbose`, `disable_progbar`, and `jit_compile` are not yet supported when using `choose_generation_strategy` with ModularBoTorchModel, dropping these arguments.\n",
      "[INFO 09-25 19:52:17] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials]). Iterations after 5 will take longer to generate due to model-fitting.\n",
      "[INFO 09-25 19:52:17] ax.service.managed_loop: Started full optimization with 15 steps.\n",
      "[INFO 09-25 19:52:17] ax.service.managed_loop: Running optimization trial 1...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:52:17] ax.service.managed_loop: Running optimization trial 2...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:52:17] ax.service.managed_loop: Running optimization trial 3...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:52:17] ax.service.managed_loop: Running optimization trial 4...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:52:17] ax.service.managed_loop: Running optimization trial 5...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:52:17] ax.service.managed_loop: Running optimization trial 6...\n",
      "[INFO 09-25 19:52:17] ax.service.managed_loop: Running optimization trial 7...\n",
      "[INFO 09-25 19:52:17] ax.service.managed_loop: Running optimization trial 8...\n",
      "[INFO 09-25 19:52:18] ax.service.managed_loop: Running optimization trial 9...\n",
      "[INFO 09-25 19:52:18] ax.service.managed_loop: Running optimization trial 10...\n",
      "[INFO 09-25 19:52:18] ax.service.managed_loop: Running optimization trial 11...\n",
      "[INFO 09-25 19:52:18] ax.service.managed_loop: Running optimization trial 12...\n",
      "[INFO 09-25 19:52:19] ax.service.managed_loop: Running optimization trial 13...\n",
      "[INFO 09-25 19:52:19] ax.service.managed_loop: Running optimization trial 14...\n",
      "[INFO 09-25 19:52:20] ax.service.managed_loop: Running optimization trial 15...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'x': 0.4824572684758933, 'y': 0.4893518126070221}\n"
     ]
    }
   ],
   "source": [
    "# Correct way to initialize AxClient for Bayesian Optimization with objective setup\n",
    "from ax.service.managed_loop import optimize\n",
    "\n",
    "def evaluation_function(parameterization):\n",
    "    x = parameterization.get(\"x\")\n",
    "    y = parameterization.get(\"y\")\n",
    "    return (x - 0.5)**2 + (y - 0.5)**2  # Example objective function\n",
    "\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=[\n",
    "        {\"name\": \"x\", \"type\": \"range\", \"bounds\": [0.0, 1.0]},\n",
    "        {\"name\": \"y\", \"type\": \"range\", \"bounds\": [0.0, 1.0]},\n",
    "    ],\n",
    "    evaluation_function=evaluation_function,\n",
    "    objective_name='objective_value',\n",
    "    minimize=True,  # Specifies that we are minimizing our objective\n",
    "    total_trials=15\n",
    ")\n",
    "\n",
    "print(\"Best Parameters:\", best_parameters)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d31528",
   "metadata": {},
   "source": [
    "## Using Ax to Optimize Within a Dataset\n",
    "\n",
    "First we need to import our libraries. The biggest new one that we need is the euclidean_distances. This is useful when trying to optimize from values within a dataset. It gives the model an 'evaluation function' to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45419177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from ax.service.managed_loop import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f466b3d8",
   "metadata": {},
   "source": [
    "Next we need to import and clean up our data. It's important to do this step as it can cause the Ax model to run into errors down the line or be unable to efficiently find the optimal value. Let's inspect our data and see what we're working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bce34d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QAgNO3(%)</th>\n",
       "      <th>Qpva(%)</th>\n",
       "      <th>Qtsc(%)</th>\n",
       "      <th>Qseed(%)</th>\n",
       "      <th>Qtot(uL/min)</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3295.000000</td>\n",
       "      <td>3295.000000</td>\n",
       "      <td>3295.000000</td>\n",
       "      <td>3295.000000</td>\n",
       "      <td>3295.000000</td>\n",
       "      <td>3295.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.904572</td>\n",
       "      <td>24.408302</td>\n",
       "      <td>8.781410</td>\n",
       "      <td>6.810531</td>\n",
       "      <td>706.117347</td>\n",
       "      <td>0.502575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.492450</td>\n",
       "      <td>8.571230</td>\n",
       "      <td>7.492459</td>\n",
       "      <td>4.503125</td>\n",
       "      <td>207.889927</td>\n",
       "      <td>0.198356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.530000</td>\n",
       "      <td>9.999518</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.498852</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>0.131345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18.529201</td>\n",
       "      <td>17.880991</td>\n",
       "      <td>3.999102</td>\n",
       "      <td>4.000384</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>0.321620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.099136</td>\n",
       "      <td>22.819075</td>\n",
       "      <td>5.848561</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>807.580000</td>\n",
       "      <td>0.491461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.399194</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>8.660701</td>\n",
       "      <td>815.000000</td>\n",
       "      <td>0.661212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>42.809816</td>\n",
       "      <td>40.001015</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>983.000000</td>\n",
       "      <td>0.910637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         QAgNO3(%)      Qpva(%)      Qtsc(%)     Qseed(%)  Qtot(uL/min)  \\\n",
       "count  3295.000000  3295.000000  3295.000000  3295.000000   3295.000000   \n",
       "mean     23.904572    24.408302     8.781410     6.810531    706.117347   \n",
       "std       8.492450     8.571230     7.492459     4.503125    207.889927   \n",
       "min       4.530000     9.999518     0.500000     0.498852    200.000000   \n",
       "25%      18.529201    17.880991     3.999102     4.000384    600.000000   \n",
       "50%      25.099136    22.819075     5.848561     6.500000    807.580000   \n",
       "75%      30.500000    32.399194    12.000000     8.660701    815.000000   \n",
       "max      42.809816    40.001015    30.500000    19.500000    983.000000   \n",
       "\n",
       "              loss  \n",
       "count  3295.000000  \n",
       "mean      0.502575  \n",
       "std       0.198356  \n",
       "min       0.131345  \n",
       "25%       0.321620  \n",
       "50%       0.491461  \n",
       "75%       0.661212  \n",
       "max       0.910637  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('AgNP_dataset.csv')\n",
    "\n",
    "# Drop or fill NaN values\n",
    "data = data.dropna()  # Option 1: Drop rows with NaN values\n",
    "# or\n",
    "data = data.fillna(data.mean())  # Option 2: Fill NaN values with column mean\n",
    "\n",
    "# Define the inputs (X) and the output (y)\n",
    "y = data['loss']\n",
    "X = data.drop('loss', axis=1)\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b128e",
   "metadata": {},
   "source": [
    "3295 is a great amount of data to be working with. We can see that our loss varies bewteen .131 and .910\n",
    "\n",
    "Next we need to create our model and test it. The optimize function uses very similar inputs as the model above, except this time it has a lot more parameters to optimize than before. Since we are optimizing within a dataset we can set the bounds to be the max and min of each input column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fa7db17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 09-25 19:52:20] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter QAgNO3(%). If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 09-25 19:52:20] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter Qpva(%). If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 09-25 19:52:20] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter Qtsc(%). If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 09-25 19:52:20] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter Qseed(%). If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 09-25 19:52:20] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter Qtot(uL/min). If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 09-25 19:52:20] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='QAgNO3(%)', parameter_type=FLOAT, range=[4.53, 42.80981595]), RangeParameter(name='Qpva(%)', parameter_type=FLOAT, range=[9.999518096, 40.00101474]), RangeParameter(name='Qtsc(%)', parameter_type=FLOAT, range=[0.5, 30.5]), RangeParameter(name='Qseed(%)', parameter_type=FLOAT, range=[0.498851653, 19.5]), RangeParameter(name='Qtot(uL/min)', parameter_type=FLOAT, range=[200.0, 983.0])], parameter_constraints=[]).\n",
      "[INFO 09-25 19:52:20] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.\n",
      "[INFO 09-25 19:52:20] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=5 num_trials=None use_batch_trials=False\n",
      "[INFO 09-25 19:52:20] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=10\n",
      "[INFO 09-25 19:52:20] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=10\n",
      "[INFO 09-25 19:52:20] ax.modelbridge.dispatch_utils: `verbose`, `disable_progbar`, and `jit_compile` are not yet supported when using `choose_generation_strategy` with ModularBoTorchModel, dropping these arguments.\n",
      "[INFO 09-25 19:52:20] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 10 trials, BoTorch for subsequent trials]). Iterations after 10 will take longer to generate due to model-fitting.\n",
      "[INFO 09-25 19:52:20] ax.service.managed_loop: Started full optimization with 100 steps.\n",
      "[INFO 09-25 19:52:20] ax.service.managed_loop: Running optimization trial 1...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:52:20] ax.service.managed_loop: Running optimization trial 2...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:52:20] ax.service.managed_loop: Running optimization trial 3...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:52:20] ax.service.managed_loop: Running optimization trial 4...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:52:20] ax.service.managed_loop: Running optimization trial 5...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:52:20] ax.service.managed_loop: Running optimization trial 6...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:52:20] ax.service.managed_loop: Running optimization trial 7...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:52:20] ax.service.managed_loop: Running optimization trial 8...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:52:20] ax.service.managed_loop: Running optimization trial 9...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:52:20] ax.service.managed_loop: Running optimization trial 10...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:52:20] ax.service.managed_loop: Running optimization trial 11...\n",
      "[INFO 09-25 19:52:21] ax.service.managed_loop: Running optimization trial 12...\n",
      "[INFO 09-25 19:52:22] ax.service.managed_loop: Running optimization trial 13...\n",
      "[INFO 09-25 19:52:22] ax.service.managed_loop: Running optimization trial 14...\n",
      "[INFO 09-25 19:52:23] ax.service.managed_loop: Running optimization trial 15...\n",
      "[INFO 09-25 19:52:23] ax.service.managed_loop: Running optimization trial 16...\n",
      "[INFO 09-25 19:52:24] ax.service.managed_loop: Running optimization trial 17...\n",
      "[INFO 09-25 19:52:25] ax.service.managed_loop: Running optimization trial 18...\n",
      "[INFO 09-25 19:52:26] ax.service.managed_loop: Running optimization trial 19...\n",
      "[INFO 09-25 19:52:26] ax.service.managed_loop: Running optimization trial 20...\n",
      "[INFO 09-25 19:52:27] ax.service.managed_loop: Running optimization trial 21...\n",
      "[INFO 09-25 19:52:28] ax.service.managed_loop: Running optimization trial 22...\n",
      "[INFO 09-25 19:52:29] ax.service.managed_loop: Running optimization trial 23...\n",
      "[INFO 09-25 19:52:30] ax.service.managed_loop: Running optimization trial 24...\n",
      "[INFO 09-25 19:52:30] ax.service.managed_loop: Running optimization trial 25...\n",
      "[INFO 09-25 19:52:31] ax.service.managed_loop: Running optimization trial 26...\n",
      "[INFO 09-25 19:52:32] ax.service.managed_loop: Running optimization trial 27...\n",
      "[INFO 09-25 19:52:32] ax.service.managed_loop: Running optimization trial 28...\n",
      "[INFO 09-25 19:52:33] ax.service.managed_loop: Running optimization trial 29...\n",
      "[INFO 09-25 19:52:35] ax.service.managed_loop: Running optimization trial 30...\n",
      "[INFO 09-25 19:52:36] ax.service.managed_loop: Running optimization trial 31...\n",
      "[INFO 09-25 19:52:36] ax.service.managed_loop: Running optimization trial 32...\n",
      "[INFO 09-25 19:52:37] ax.service.managed_loop: Running optimization trial 33...\n",
      "[INFO 09-25 19:52:38] ax.service.managed_loop: Running optimization trial 34...\n",
      "[INFO 09-25 19:52:38] ax.service.managed_loop: Running optimization trial 35...\n",
      "[INFO 09-25 19:52:39] ax.service.managed_loop: Running optimization trial 36...\n",
      "[INFO 09-25 19:52:40] ax.service.managed_loop: Running optimization trial 37...\n",
      "[INFO 09-25 19:52:40] ax.service.managed_loop: Running optimization trial 38...\n",
      "[INFO 09-25 19:52:41] ax.service.managed_loop: Running optimization trial 39...\n",
      "[INFO 09-25 19:52:42] ax.service.managed_loop: Running optimization trial 40...\n",
      "[INFO 09-25 19:52:42] ax.service.managed_loop: Running optimization trial 41...\n",
      "[INFO 09-25 19:52:45] ax.service.managed_loop: Running optimization trial 42...\n",
      "[INFO 09-25 19:52:46] ax.service.managed_loop: Running optimization trial 43...\n",
      "[INFO 09-25 19:52:46] ax.service.managed_loop: Running optimization trial 44...\n",
      "[INFO 09-25 19:52:49] ax.service.managed_loop: Running optimization trial 45...\n",
      "[INFO 09-25 19:52:52] ax.service.managed_loop: Running optimization trial 46...\n",
      "[INFO 09-25 19:52:53] ax.service.managed_loop: Running optimization trial 47...\n",
      "[INFO 09-25 19:52:56] ax.service.managed_loop: Running optimization trial 48...\n",
      "[INFO 09-25 19:52:59] ax.service.managed_loop: Running optimization trial 49...\n",
      "[INFO 09-25 19:53:00] ax.service.managed_loop: Running optimization trial 50...\n",
      "[INFO 09-25 19:53:02] ax.service.managed_loop: Running optimization trial 51...\n",
      "[INFO 09-25 19:53:04] ax.service.managed_loop: Running optimization trial 52...\n",
      "[INFO 09-25 19:53:08] ax.service.managed_loop: Running optimization trial 53...\n",
      "[INFO 09-25 19:53:11] ax.service.managed_loop: Running optimization trial 54...\n",
      "[INFO 09-25 19:53:13] ax.service.managed_loop: Running optimization trial 55...\n",
      "[INFO 09-25 19:53:15] ax.service.managed_loop: Running optimization trial 56...\n",
      "[INFO 09-25 19:53:18] ax.service.managed_loop: Running optimization trial 57...\n",
      "[INFO 09-25 19:53:20] ax.service.managed_loop: Running optimization trial 58...\n",
      "[INFO 09-25 19:53:23] ax.service.managed_loop: Running optimization trial 59...\n",
      "[INFO 09-25 19:53:25] ax.service.managed_loop: Running optimization trial 60...\n",
      "[INFO 09-25 19:53:30] ax.service.managed_loop: Running optimization trial 61...\n",
      "[INFO 09-25 19:53:34] ax.service.managed_loop: Running optimization trial 62...\n",
      "[INFO 09-25 19:53:37] ax.service.managed_loop: Running optimization trial 63...\n",
      "[INFO 09-25 19:53:39] ax.service.managed_loop: Running optimization trial 64...\n",
      "[INFO 09-25 19:53:41] ax.service.managed_loop: Running optimization trial 65...\n",
      "[INFO 09-25 19:53:46] ax.service.managed_loop: Running optimization trial 66...\n",
      "[INFO 09-25 19:53:48] ax.service.managed_loop: Running optimization trial 67...\n",
      "[INFO 09-25 19:53:50] ax.service.managed_loop: Running optimization trial 68...\n",
      "[INFO 09-25 19:53:54] ax.service.managed_loop: Running optimization trial 69...\n",
      "[INFO 09-25 19:53:58] ax.service.managed_loop: Running optimization trial 70...\n",
      "[INFO 09-25 19:54:01] ax.service.managed_loop: Running optimization trial 71...\n",
      "[INFO 09-25 19:54:04] ax.service.managed_loop: Running optimization trial 72...\n",
      "[INFO 09-25 19:54:08] ax.service.managed_loop: Running optimization trial 73...\n",
      "[INFO 09-25 19:54:12] ax.service.managed_loop: Running optimization trial 74...\n",
      "[INFO 09-25 19:54:15] ax.service.managed_loop: Running optimization trial 75...\n",
      "[INFO 09-25 19:54:18] ax.service.managed_loop: Running optimization trial 76...\n",
      "[INFO 09-25 19:54:22] ax.service.managed_loop: Running optimization trial 77...\n",
      "[INFO 09-25 19:54:26] ax.service.managed_loop: Running optimization trial 78...\n",
      "[INFO 09-25 19:54:29] ax.service.managed_loop: Running optimization trial 79...\n",
      "[INFO 09-25 19:54:34] ax.service.managed_loop: Running optimization trial 80...\n",
      "[INFO 09-25 19:54:37] ax.service.managed_loop: Running optimization trial 81...\n",
      "[INFO 09-25 19:54:42] ax.service.managed_loop: Running optimization trial 82...\n",
      "[INFO 09-25 19:54:46] ax.service.managed_loop: Running optimization trial 83...\n",
      "[INFO 09-25 19:54:51] ax.service.managed_loop: Running optimization trial 84...\n",
      "[INFO 09-25 19:54:56] ax.service.managed_loop: Running optimization trial 85...\n",
      "[INFO 09-25 19:55:00] ax.service.managed_loop: Running optimization trial 86...\n",
      "[INFO 09-25 19:55:04] ax.service.managed_loop: Running optimization trial 87...\n",
      "[INFO 09-25 19:55:10] ax.service.managed_loop: Running optimization trial 88...\n",
      "[INFO 09-25 19:55:14] ax.service.managed_loop: Running optimization trial 89...\n",
      "[INFO 09-25 19:55:17] ax.service.managed_loop: Running optimization trial 90...\n",
      "[INFO 09-25 19:55:20] ax.service.managed_loop: Running optimization trial 91...\n",
      "[INFO 09-25 19:55:22] ax.service.managed_loop: Running optimization trial 92...\n",
      "[INFO 09-25 19:55:25] ax.service.managed_loop: Running optimization trial 93...\n",
      "[INFO 09-25 19:55:29] ax.service.managed_loop: Running optimization trial 94...\n",
      "[INFO 09-25 19:55:31] ax.service.managed_loop: Running optimization trial 95...\n",
      "[INFO 09-25 19:55:33] ax.service.managed_loop: Running optimization trial 96...\n",
      "[INFO 09-25 19:55:37] ax.service.managed_loop: Running optimization trial 97...\n",
      "[INFO 09-25 19:55:40] ax.service.managed_loop: Running optimization trial 98...\n",
      "[INFO 09-25 19:55:44] ax.service.managed_loop: Running optimization trial 99...\n",
      "[INFO 09-25 19:55:47] ax.service.managed_loop: Running optimization trial 100...\n"
     ]
    }
   ],
   "source": [
    "def evaluation_function(parameterization):\n",
    "    # Convert parameterization to a DataFrame\n",
    "    param_df = pd.DataFrame([parameterization])\n",
    "    \n",
    "    # Compute the distance between the parameterization and all rows in the test set\n",
    "    distances = euclidean_distances(param_df, X)\n",
    "    \n",
    "    # Find the index of the closest row\n",
    "    closest_index = np.argmin(distances)\n",
    "    \n",
    "    # Get the corresponding output value\n",
    "    output_value = y.iloc[closest_index]\n",
    "    \n",
    "    # Return the output value as the objective\n",
    "    return {\"objective_value\": output_value}\n",
    "\n",
    "# Define the parameters to optimize\n",
    "parameters = [\n",
    "    {\"name\": \"QAgNO3(%)\", \"type\": \"range\", \"bounds\": [float(X['QAgNO3(%)'].min()), float(X['QAgNO3(%)'].max())]},\n",
    "    {\"name\": \"Qpva(%)\", \"type\": \"range\", \"bounds\": [float(X['Qpva(%)'].min()), float(X['Qpva(%)'].max())]},\n",
    "    {\"name\": \"Qtsc(%)\", \"type\": \"range\", \"bounds\": [float(X['Qtsc(%)'].min()), float(X['Qtsc(%)'].max())]},\n",
    "    {\"name\": \"Qseed(%)\", \"type\": \"range\", \"bounds\": [float(X['Qseed(%)'].min()), float(X['Qseed(%)'].max())]},\n",
    "    {\"name\": \"Qtot(uL/min)\", \"type\": \"range\", \"bounds\": [float(X['Qtot(uL/min)'].min()), float(X['Qtot(uL/min)'].max())]}\n",
    "]\n",
    "\n",
    "# Run the optimization\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=parameters,\n",
    "    evaluation_function=evaluation_function,\n",
    "    objective_name='objective_value',\n",
    "    minimize=True,\n",
    "    total_trials=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796bd41b",
   "metadata": {},
   "source": [
    "Finally, let's see what our best parameters are. Keep in mind this can be influenced by the number of trials that the model is running. If it has a very low amount it may not have the chance to find the optimal parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25856cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'QAgNO3(%)': 36.95316824582501, 'Qpva(%)': 14.498258205867465, 'Qtsc(%)': 13.72584530183296, 'Qseed(%)': 0.498851653, 'Qtot(uL/min)': 887.5740031672151}\n",
      "Best Objective Value: {'objective_value': 0.15935830981454724}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", best_parameters)\n",
    "print(\"Best Objective Value:\", values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece76b96",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization with Ax\n",
    "\n",
    "In this section we will train a random forest regressor on predicting the loss value from the input data that we used before. We will then use an Ax model to tune the hyperparameters of this model.\n",
    "\n",
    "First we will import the libraries required. We will be using a random forest model from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1ee3a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ax.service.managed_loop import optimize\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923f2ea",
   "metadata": {},
   "source": [
    "Next we will load the same dataset. Everything here is the same as before except we are going to split it into train/test splits in order to be able to correctly score our random forest regression model. It's important to specify the random_state in order to have reproducability between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "531b87db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAgNO3(%)       0\n",
      "Qpva(%)         0\n",
      "Qtsc(%)         0\n",
      "Qseed(%)        0\n",
      "Qtot(uL/min)    0\n",
      "loss            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('AgNP_dataset.csv')\n",
    "\n",
    "\n",
    "# Check for NaN values\n",
    "print(data.isna().sum())\n",
    "\n",
    "# Drop or fill NaN values\n",
    "data = data.dropna()  # Option 1: Drop rows with NaN values\n",
    "# or\n",
    "data = data.fillna(data.mean())  # Option 2: Fill NaN values with column mean\n",
    "\n",
    "# Define the inputs (X) and the output (y)\n",
    "y = data['loss']\n",
    "X = data.drop('loss', axis=1)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92d190e",
   "metadata": {},
   "source": [
    "Next we will train and score our model. The details are covered in the random_forest notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "933ec231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0038889048983534588\n"
     ]
    }
   ],
   "source": [
    "# Train a RandomForestRegressor model\n",
    "model = RandomForestRegressor(random_state=42, n_estimators=10, max_depth=5, min_samples_split=5, min_samples_leaf=2)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2344d638",
   "metadata": {},
   "source": [
    "We have a pretty good MSE to start! However, we can do better than it. We will now use an Ax model to tune the parameters of this random forest model. Much like the optimization above, we list out the parameters we want to tune. however, our evaluation function trains and fits the random forest model with the found parameters, then uses the MSE score to find the optimal values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b834e9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 09-25 19:55:52] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='n_estimators', parameter_type=INT, range=[10, 200]), RangeParameter(name='max_depth', parameter_type=INT, range=[5, 50]), RangeParameter(name='min_samples_split', parameter_type=INT, range=[2, 20]), RangeParameter(name='min_samples_leaf', parameter_type=INT, range=[1, 10])], parameter_constraints=[]).\n",
      "[INFO 09-25 19:55:52] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.\n",
      "[INFO 09-25 19:55:52] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=4 num_trials=None use_batch_trials=False\n",
      "[INFO 09-25 19:55:52] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=8\n",
      "[INFO 09-25 19:55:52] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=8\n",
      "[INFO 09-25 19:55:52] ax.modelbridge.dispatch_utils: `verbose`, `disable_progbar`, and `jit_compile` are not yet supported when using `choose_generation_strategy` with ModularBoTorchModel, dropping these arguments.\n",
      "[INFO 09-25 19:55:52] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 8 trials, BoTorch for subsequent trials]). Iterations after 8 will take longer to generate due to model-fitting.\n",
      "[INFO 09-25 19:55:52] ax.service.managed_loop: Started full optimization with 30 steps.\n",
      "[INFO 09-25 19:55:52] ax.service.managed_loop: Running optimization trial 1...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:55:52] ax.service.managed_loop: Running optimization trial 2...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:55:53] ax.service.managed_loop: Running optimization trial 3...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:55:53] ax.service.managed_loop: Running optimization trial 4...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:55:53] ax.service.managed_loop: Running optimization trial 5...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:55:53] ax.service.managed_loop: Running optimization trial 6...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:55:53] ax.service.managed_loop: Running optimization trial 7...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:55:53] ax.service.managed_loop: Running optimization trial 8...\n",
      "/opt/miniconda3/envs/mainENV/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning:\n",
      "\n",
      "Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "\n",
      "[INFO 09-25 19:55:54] ax.service.managed_loop: Running optimization trial 9...\n",
      "[INFO 09-25 19:55:54] ax.service.managed_loop: Running optimization trial 10...\n",
      "[INFO 09-25 19:55:55] ax.service.managed_loop: Running optimization trial 11...\n",
      "[INFO 09-25 19:55:55] ax.service.managed_loop: Running optimization trial 12...\n",
      "[INFO 09-25 19:55:56] ax.service.managed_loop: Running optimization trial 13...\n",
      "[INFO 09-25 19:55:57] ax.service.managed_loop: Running optimization trial 14...\n",
      "[INFO 09-25 19:55:57] ax.service.managed_loop: Running optimization trial 15...\n",
      "[INFO 09-25 19:55:58] ax.service.managed_loop: Running optimization trial 16...\n",
      "[INFO 09-25 19:55:58] ax.service.managed_loop: Running optimization trial 17...\n",
      "[INFO 09-25 19:55:59] ax.service.managed_loop: Running optimization trial 18...\n",
      "[INFO 09-25 19:56:00] ax.service.managed_loop: Running optimization trial 19...\n",
      "[INFO 09-25 19:56:01] ax.service.managed_loop: Running optimization trial 20...\n",
      "[INFO 09-25 19:56:02] ax.service.managed_loop: Running optimization trial 21...\n",
      "[INFO 09-25 19:56:03] ax.service.managed_loop: Running optimization trial 22...\n",
      "[INFO 09-25 19:56:04] ax.service.managed_loop: Running optimization trial 23...\n",
      "[INFO 09-25 19:56:05] ax.service.managed_loop: Running optimization trial 24...\n",
      "[INFO 09-25 19:56:06] ax.service.managed_loop: Running optimization trial 25...\n",
      "[INFO 09-25 19:56:07] ax.service.managed_loop: Running optimization trial 26...\n",
      "[INFO 09-25 19:56:08] ax.service.managed_loop: Running optimization trial 27...\n",
      "[INFO 09-25 19:56:09] ax.service.managed_loop: Running optimization trial 28...\n",
      "[INFO 09-25 19:56:10] ax.service.managed_loop: Running optimization trial 29...\n",
      "[INFO 09-25 19:56:11] ax.service.managed_loop: Running optimization trial 30...\n"
     ]
    }
   ],
   "source": [
    "def evaluation_function(parameterization):\n",
    "    # Extract hyperparameters from parameterization\n",
    "    n_estimators = parameterization.get(\"n_estimators\")\n",
    "    max_depth = parameterization.get(\"max_depth\")\n",
    "    min_samples_split = parameterization.get(\"min_samples_split\")\n",
    "    min_samples_leaf = parameterization.get(\"min_samples_leaf\")\n",
    "    \n",
    "    # Train the model with given hyperparameters\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and calculate MSE on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "    # Return the MSE as the objective value to minimize\n",
    "    return {\"objective_value\": mse}\n",
    "\n",
    "\n",
    "\n",
    "# Define the parameters to optimize\n",
    "parameters = [\n",
    "    {\"name\": \"n_estimators\", \"type\": \"range\", \"bounds\": [10, 200], \"value_type\": \"int\"},\n",
    "    {\"name\": \"max_depth\", \"type\": \"range\", \"bounds\": [5, 50], \"value_type\": \"int\"},\n",
    "    {\"name\": \"min_samples_split\", \"type\": \"range\", \"bounds\": [2, 20], \"value_type\": \"int\"},\n",
    "    {\"name\": \"min_samples_leaf\", \"type\": \"range\", \"bounds\": [1, 10], \"value_type\": \"int\"}\n",
    "]\n",
    "\n",
    "# Run the optimization\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=parameters,\n",
    "    evaluation_function=evaluation_function,\n",
    "    objective_name='objective_value',\n",
    "    minimize=True,\n",
    "    total_trials=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f6b6d5",
   "metadata": {},
   "source": [
    "Now lets check our parameters and objective value. As you can see, the MSE score went down by a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "921febb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 44, 'max_depth': 44, 'min_samples_split': 20, 'min_samples_leaf': 2}\n",
      "Original Score: 0.0038889048983534588\n",
      "Hyperparameter Tuned Score {'objective_value': 0.0020091875679146024}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", best_parameters)\n",
    "print(\"Original Score:\", mse)\n",
    "print(\"Hyperparameter Tuned Score\", values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f86161",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we used Bayesian Optimization with the Ax library to efficiently find the minimum of a synthetic function, optimize within a dataset, and perform hyperparameter tuning on another model. Ax facilitated the process of defining the experiment, running the optimization, and analyzing the results. \n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Ax Documentation](https://ax.dev/)\n",
    "- [More on Bayesian Optimization](https://arxiv.org/abs/1807.02811)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10 (MaterialsInformatics)",
   "language": "python",
   "name": "materialsinformatics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
