{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sp8rks/MaterialsInformatics/blob/main/worked_examples/hyperparameter_opt/hyperparameter_opt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELFcx4WgrdXV"
   },
   "source": [
    "# Grid vs. Random Search Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrA6ZaIYhd7l"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyPdG8IxhbwG"
   },
   "source": [
    "### Installation\n",
    "\n",
    "For this project we will need to install matbench (for datasets) and CBFV (to create composition based feature vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Tt5zD4rfeavx",
    "outputId": "451fa766-3241-4b77-f7db-bfbe464b98dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install python==3.9\n",
    "!pip install CBFV\n",
    "!pip install scikit-learn==1.2.2\n",
    "!pip install monty==2021.8.17\n",
    "!pip install matminer==0.7.4\n",
    "!pip install --upgrade pip\n",
    "!pip install scikit-learn==0.24.2\n",
    "!pip install \"matbench @ git+https://github.com/hackingmaterials/matbench.git\"\n",
    "# !pip install matbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVTbZicohiDd"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yXjOg8s6ehcm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taylo\\AppData\\Local\\Temp\\ipykernel_6652\\211353170.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matbench'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m randint\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatbench\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbench\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatbenchBenchmark\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCBFV\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_features\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matbench'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "from matbench.bench import MatbenchBenchmark\n",
    "from CBFV.composition import generate_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WT51Iu3zhjJ8"
   },
   "source": [
    "### Data\n",
    "\n",
    "Load the data from MatBench. This example will use the matbench_expt_is_metal dataset. The first tast is selected and loaded as well along with the first fold of the dataset. The data is split into train and test splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7hygIC8fGb9",
    "outputId": "98d10ff3-7c3f-4e1d-a1b2-4aa727ae9562"
   },
   "outputs": [],
   "source": [
    "mb = MatbenchBenchmark(subset=[\"matbench_expt_is_metal\"])\n",
    "task = list(mb.tasks)[0]\n",
    "task.load()\n",
    "fold0 = task.folds[0]\n",
    "train_inputs, train_outputs = task.get_train_and_val_data(fold0)\n",
    "test_inputs, test_outputs = task.get_test_data(fold0, include_target=True)\n",
    "print(train_inputs[0:2], train_outputs[0:2])\n",
    "print(train_outputs.shape, test_outputs.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the inputs and outputs of the training set. This outputs different statistics for the dataframes. This is helpful for getting a quick glance at the nature of our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_V5GF1n6gw6V",
    "outputId": "6b1af8ec-9c01-4644-de20-14b86f84a3ea"
   },
   "outputs": [],
   "source": [
    "train_inputs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outputs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets up our train and test dataframes. Additionally this converts our data into copmosition based feature vectors using the generate_features from the CBFV library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "bj2GVnAmgoz2",
    "outputId": "734cbf72-cf59-4a40-fab9-a195f499e3c7"
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({\"formula\": train_inputs, \"target\": train_outputs})\n",
    "test_df = pd.DataFrame({\"formula\": test_inputs, \"target\": test_outputs})\n",
    "train_df\n",
    "\n",
    "X_train, y_train, _, _ = generate_features(train_df)\n",
    "print(X_train.shape)\n",
    "X_test, y_test, _, _ = generate_features(test_df)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuEoRyF_iRsV"
   },
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rq79CBx_iVnl"
   },
   "source": [
    "We can do hyperparameter tuning in different ways. Two common ways are grid search (less efficient) and random search (more efficient). Below are examples taken/modified from the website https://www.geeksforgeeks.org/hyperparameter-tuning/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will grid search over a logistic regression classifier. This is a model taken from scikit-learn. Grid search is slower as it tries every possible combination of parameters. Despite slowing down the model, this means that it can be deterministic and output the same results every single time. Be aware that the amount of features and parameters you feed the model will exponentially increase the time it takes for the model to find the best parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DpTTU0FV8vI",
    "outputId": "1157a51d-ba66-42aa-a173-71329c8df3b2"
   },
   "outputs": [],
   "source": [
    "#Grid search first using logistic regression classifier model\n",
    "\n",
    "# Creating the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "  \n",
    "# Instantiating logistic regression classifier\n",
    "# https://stats.stackexchange.com/a/184026/293880\n",
    "logreg = LogisticRegression(max_iter=100)\n",
    "  \n",
    "# Instantiating the GridSearchCV object\n",
    "logreg_grid = GridSearchCV(logreg, param_grid, cv = 5)\n",
    "  \n",
    "logreg_grid.fit(X_train, y_train)\n",
    "  \n",
    "# Print the tuned parameters and score\n",
    "print(\"Grid tuned Logistic Regression Parameters: {}\".format(logreg_grid.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, lets try a random search. Random search is much faster than grid search as it randomly draws samples from within specified regions. It evaluates a fixed number of these samples which lets you control how fast it will run. However, it may miss the best combination of parameters from not finding it through random sampling. This method increases it's speed and efficiency by quite a large margin at the cost of sacrificing some of the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtZMtOBiV8vJ",
    "outputId": "1e27eb3b-6675-4a83-8f44-332994b4eba9"
   },
   "outputs": [],
   "source": [
    "#Now we can try random search with logistic regression\n",
    "  \n",
    "# Creating the hyperparameter grid \n",
    "param_dist = {\"C\": randint(-5,15)}\n",
    "  \n",
    "# Instantiating Decision Tree classifier\n",
    "logreg = LogisticRegression()\n",
    "  \n",
    "# Instantiating RandomizedSearchCV object\n",
    "logreg_random = RandomizedSearchCV(logreg, param_dist, cv = 5)\n",
    "  \n",
    "logreg_random.fit(X_train, y_train)\n",
    "  \n",
    "# Print the tuned parameters and score\n",
    "print(\"Random tuned Logistic Regression Parameters: {}\".format(logreg_random.best_params_))\n",
    "print(\"Best score is {}\".format(logreg_random.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0jdvkp_V8vJ"
   },
   "source": [
    "We can do the same grid vs random search with another model, like a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwxtaSddV8vJ",
    "outputId": "5a4832af-c902-4d12-a67a-eb50f9e31648"
   },
   "outputs": [],
   "source": [
    "#grid search for decision tree hyperparameters\n",
    "  \n",
    "# Creating the hyperparameter grid \n",
    "param_grid = {\"max_depth\": range(1,10),\n",
    "              \"max_features\": range(1,10),\n",
    "              \"min_samples_leaf\": range(1,10),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# Instantiating Decision Tree classifier\n",
    "tree = DecisionTreeClassifier()\n",
    "  \n",
    "# Instantiating GridSearchCV object\n",
    "tree_grid = GridSearchCV(tree, param_grid, cv = 5)\n",
    "  \n",
    "tree_grid.fit(X_train, y_train)\n",
    "  \n",
    "# Print the tuned parameters and score\n",
    "print(\"Grid tuned Decision Tree Parameters: {}\".format(tree_grid.best_params_))\n",
    "print(\"Best score is {}\".format(tree_grid.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NdIPhe3WV8vK",
    "outputId": "10490388-2cca-4416-c249-a249e44d161c"
   },
   "outputs": [],
   "source": [
    "#random search for decision tree hyperparameters\n",
    "  \n",
    "# Creating the hyperparameter grid \n",
    "param_dist = {\"max_depth\": randint(1,10),\n",
    "              \"max_features\": randint(1,10),\n",
    "              \"min_samples_leaf\": randint(1,10),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# Instantiating Decision Tree classifier\n",
    "tree = DecisionTreeClassifier()\n",
    "  \n",
    "# Instantiating RandomizedSearchCV object\n",
    "tree_random = RandomizedSearchCV(tree, param_dist, cv = 5)\n",
    "  \n",
    "tree_random.fit(X_train, y_train)\n",
    "  \n",
    "# Print the tuned parameters and score\n",
    "print(\"Random tuned Decision Tree Parameters: {}\".format(tree_random.best_params_))\n",
    "print(\"Best score is {}\".format(tree_random.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter optimization will show up in other notebooks and homeworks. Grid search and random search can be fine for certain tasks, but when the models and hyperparameters become more complicated and numerous it may be advantageous to explore other options such as Bayesian Optimization or Genetic Algorithms."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "materials_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MatInformatics2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
