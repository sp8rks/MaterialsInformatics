{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Comparing Models\n",
    "\n",
    "How do we know if our model is accurate? How can we determine which variables we should model and the relative costs of including those variables? This notebook will discuss tools for assessing and comparing models. This notebook is intended to accompany a lecture and is thus not as verbose as other notebooks.\n",
    "\n",
    "#### Video\n",
    "\n",
    "https://www.youtube.com/watch?v=_kp19PIVuXc&list=PLL0SWcFqypCl4lrzk1dMWwTUrzQZFt7y0&index=18 (Splitting data in train/val/test splits)\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# some settings for making plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "plt.style.use(\"ggplot\")\n",
    "palette = [\"#00B0F6\", \"#F8766D\", \"#00BF7D\", \"#A3A500\", \"#E76BF3\"]\n",
    "plt.rc('axes', prop_cycle=(cycler('color', palette)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading the data and taking a look at the distribution of data, which predictors do we think are going to be useful? Is it always obvious how useful a predictor will be? Should we discount a predictor just because it doesn't \"look\" correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some data\n",
    "df = pd.read_csv(\"dummy_data.csv\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20, 3), sharey=True)\n",
    "\n",
    "for i in range(4):\n",
    "    ax[i].set_xlabel(df.columns[i])\n",
    "    ax[i].scatter(df.iloc[:,i], df.Target, color='k', marker='.')\n",
    "ax[0].set_ylabel(\"Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we are trying to build a predictive model to help us find an even higher value than we have observed thus far. To assess this we should remove the to XX% of our dataset and see if we can use lower data to predict higher data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's grab the top 15% of data to serve as a test set\n",
    "test_df = df.sort_values(\"Target\").iloc[-10:]\n",
    "train_df = df.sort_values(\"Target\").iloc[:90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Var3` and `Var2` seem to have a strong relationships with or `Target`, so we can start with that for our model and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols('Target ~ Var3 + Var2', data=train_df).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model seems to perform ok on the training data, but let's see how well it does on the testing data that we excluded from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.get_prediction(train_df) \n",
    "test_pred = model.get_prediction(test_df)\n",
    "\n",
    "print(\"Train MAE: \", mean_absolute_error(train_df.Target.values, train_pred.predicted))\n",
    "print(\"Test MAE: \", mean_absolute_error(test_df.Target.values, test_pred.predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would appear that we are greatly overfitting our data! Our test set error is approximately 20% of the span of the target data range! This is model is unlikely to help us find new, better material that exceed our current data range!\n",
    "\n",
    "While we could continue to iterate with new models, we start to run into an issue of fitting the testing data. As we continue to update our model to improve test set performance we are in a sense optimizing for the test set and thus we no longer have an objective measure of model performance on unseen data! One solution would be to create a third split that we can use for validation tests, but then we will be further limiting our training data and thus likely our model performance. Additionally, our performance on the validation set might depend on the split itself, which may lead us astray in terms of optimal parameters.\n",
    "\n",
    "A better approach to model validation is to split our training data into K groups and then iteratively fit K-1 groups and predict on the last. This will allow us to use more our training data and give us an idea of the amount of variability in predictive performance as a function of data splitting. It is still a good idea to hold out a test set as a final predictive check as that will be your objective performance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse_scores = []\n",
    "val_mse_scores = []\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=False)\n",
    "\n",
    "for split in kf.split(train_df):\n",
    "    train_fold, val_fold = split\n",
    "    \n",
    "    train_fold_df = train_df.iloc[train_fold]\n",
    "    val_fold_df = train_df.iloc[val_fold]\n",
    "\n",
    "    model = smf.ols('Target ~ Var3+Var2', data=train_fold_df).fit()\n",
    "\n",
    "    train_mse = mean_squared_error(train_fold_df.Target, model.predict(train_fold_df))\n",
    "    train_mse_scores.append(train_mse)\n",
    "    \n",
    "    val_mse = mean_squared_error(val_fold_df.Target, model.predict(val_fold_df))\n",
    "    val_mse_scores.append(val_mse)\n",
    "\n",
    "print(\"Mean Train MSE\",np.mean(train_mse_scores))\n",
    "print(\"Mean Validation MSE\", np.mean(val_mse_scores))\n",
    "\n",
    "plt.plot(train_mse_scores, marker='o', label='Train')\n",
    "plt.plot(val_mse_scores, marker='o', label='Val')\n",
    "plt.legend(frameon=False, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above we iterated through 10 data splits fitting and testing the data against the remaining validation set. We see that there is quite a bit of variability in the validation outcome, but on average we get an MSE score of about 37. We could manually update the model and try again, but it would be more efficint to try different models programmatically.\n",
    "\n",
    "In the code below, we try three models with different exponential terms on the `Var3` predictor. We then plot the average validation and training MSE scores as a function of the exponetial term. Through this simple look we see how we can quickly iterate through and compare different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "for i in [1,2,3]:\n",
    "\n",
    "    def fit_ols(df):\n",
    "        def expon(x):\n",
    "            return x**i\n",
    "        model = smf.ols('Target ~ expon(Var3)+Var2+Var1', data=df).fit()\n",
    "        return model\n",
    "\n",
    "    train_mse_scores = []\n",
    "    val_mse_scores = []\n",
    "\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    for split in kf.split(train_df):\n",
    "        train_fold, val_fold = split\n",
    "        \n",
    "        train_fold_df = train_df.iloc[train_fold]\n",
    "        val_fold_df = train_df.iloc[val_fold]\n",
    "\n",
    "        model = fit_ols(train_fold_df)\n",
    "\n",
    "        train_mse = mean_squared_error(train_fold_df.Target, model.predict(train_fold_df))\n",
    "        train_mse_scores.append(train_mse)\n",
    "        \n",
    "        val_mse = mean_squared_error(val_fold_df.Target, model.predict(val_fold_df))\n",
    "        val_mse_scores.append(val_mse)\n",
    "\n",
    "    train_scores.append(np.mean(train_mse_scores))\n",
    "    val_scores.append(np.mean(val_mse_scores))\n",
    "\n",
    "# print(\"Mean Train MSE\",np.mean(train_mse_scores))\n",
    "# print(\"Mean Val MSE\", np.mean(val_mse_scores))\n",
    "\n",
    "plt.plot(train_scores, marker='o', label='Train')\n",
    "plt.plot(val_scores, marker='o', label='Val')\n",
    "plt.legend(frameon=False, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xticks([0, 1, 2], [1,2,3])\n",
    "plt.xlabel(\"Var3 Exponent Value\")\n",
    "plt.ylabel(\"Mean K-fold MSE Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results it becomes clear that we should exponentiate `Var3` and also clear how we might proceed with finding new models programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now you try it!\n",
    "- Use the dataset from before and compare the performance of a linear regression model using K-Fold vs train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Write the code below"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10 (MaterialsInformatics)",
   "language": "python",
   "name": "materialsinformatics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
