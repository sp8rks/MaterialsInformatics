{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composition Based Feature Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most materials informatics models rely on small amounts of data, we need to rely on feature engineering in order to inject domain knowledge into our materials representations. One of the most simple ways to do so is through the mighty Composition Based Feature Vector.\n",
    "\n",
    "My research group created the `CBFV` package to make it super easy to do composition based feature vectors! In order to follow along with the in class demo (https://github.com/sp8rks/MaterialsInformatics/blob/main/worked_examples/CBFV_example/CBFV_example.ipynb) you will need to go to do the following:\n",
    "(1) open miniconda\n",
    "(2) activate your MatInformatics python env `conda activate MatInformatics`\n",
    "(3) install CBFV package `pip install CBFV` (read more at https://pypi.org/project/CBFV/)\n",
    "\n",
    "#### Video\n",
    "\n",
    "https://www.youtube.com/watch?v=JctWNNdI9Jc&list=PLL0SWcFqypCl4lrzk1dMWwTUrzQZFt7y0&index=11 (Composition-based feature vector)\n",
    "\n",
    "Google Colab link https://colab.research.google.com/drive/1z33jThe8YzrDktLidJpsmWOzoSN28pfn?usp=sharing\n",
    "\n",
    "## Setup\n",
    "\n",
    "Let's start by creating some dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CBFV import composition\n",
    "import pandas as pd\n",
    "\n",
    "data = [['Si1O2', 10], ['Al2O3', 15], ['Hf1C1Zr1', 14]]\n",
    "#this next step is important!! The CBFV composition.generate_features() function \n",
    "#requires an input dataframe with a column named 'formula' and another column named 'target'\n",
    "df = pd.DataFrame(data, columns=['formula', 'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do our simplest CBFV featurization and convert our data into a 'one hot encoding' vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, formulae, skipped = composition.generate_features(df, elem_prop='onehot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at our input, the X variable, we'll see that the formulae strings are now converted to numerical values that are suitable for machine learning models.\n",
    "\n",
    "For our first representation, the avg columns represent the *fractional encoding* of the elements. For example, SiO2 is 2/3 Oxygen, 1/3 Silicon so we see 0.66667 in the avg_8 (atomic number 8, Oxygen) position and we see 0.33333 in the 14th column (atomic number 14, Silicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TIP! Open up the X variable in the Data Wrangler extension to see all the columns since they are truncated below\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Element property feature vectors\n",
    "The one hot encoding is a super simple way to encode the formula. It doesn't include any information about the actual chemistry other than the formula. We know that other features should matter. For example, the melting point or ionic size or number of valence electrons etc should be important and useful in relating these materials to their material properties. \n",
    "\n",
    "Let's take a look at another featurization technique, the `magpie` feature vector, that encodes more chemical information beyond just one hot encoding.\n",
    "\n",
    "Read more about `magpie` here in the original article https://doi.org/10.1038/npjcompumats.2016.28\n",
    "\n",
    "Essentially, the feature vector is created by taking information from the individual elements and then combining the information from these individual elements based on their elemental ratio in the chemical formula. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, formulae, skipped = composition.generate_features(df, \n",
    "    elem_prop='magpie')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several others too including one of my favorites `olinyky` which we named after Anton Oliynyk, a great chemist who put it together. https://hunter.cuny.edu/people/anton-oliynyk/ Another is `jarvis` which came from the good folks at NIST. Read their article here https://doi.org/10.1038/s41524-020-00440-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, formulae, skipped = composition.generate_features(df, \n",
    "    elem_prop='oliynyk')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurization based on scientific literature\n",
    "There are also some really cool approaches for embedding domain knowledge. For example, `mat2vec` is a clever approach that uses a *natural language processing* tool known as word embeddings to create a feature vector based on scientific literature. You can read about it here https://doi.org/10.1038/s41586-019-1335-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, formulae, skipped = composition.generate_features(df, \n",
    "    elem_prop='mat2vec')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these representations which can be hundreds of columns in length, we see that it went from a simple string like 'SiO2' and was turned into something rather complicated. These representations are less interpretable than a simple chemical formula, but are now mathematical vectors that represent the materials and do so with varying degrees of domain knowledge. In 2020 my group published a careful study that asked whether or not this domain knowledge was actually necessary or helpful in predicting materials properties. We essentially found that the domain knowledge does improve predictions, but as the data increases this advantage slowly disappears. \n",
    "\n",
    "You can read the article here https://doi.org/10.1007/s40192-020-00179-z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now you try it!\n",
    "Generate a list of compounds you are interested in, look up their properties, and then featurize this data with your choice of feature set to create an X input and a y target label. Try adding a broken chemical formula that includes an abbreviation for an element that doesn't exist and then see what you find in the skipped variable output by the `CBFV.generate_features` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10 (MaterialsInformatics)",
   "language": "python",
   "name": "materialsinformatics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
